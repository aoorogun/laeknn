Metadata-Version: 2.1
Name: laeknn
Version: 0.1.0
Summary: Locally Adaptive Evidential K‑Nearest Neighbours (LAE‑KNN) algorithm
Home-page: https://endowtech.example.com/laeknn
Author: Okunola Orogun
Author-email: Okunola Orogun <no-reply@endowtech.com>
Maintainer-email: Endow Tech Limited <no-reply@endowtech.com>
License: MIT
Project-URL: Homepage, https://endowtech.example.com/laeknn
Project-URL: Source, https://endowtech.example.com/laeknn
Keywords: machine learning,knn,classification,uncertainty
Classifier: Development Status :: 3 - Alpha
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE

# laeknn

**Locally Adaptive Evidential K‑Nearest Neighbours (LAE‑KNN)** is a machine learning library
implementing a novel variant of the classical K‑Nearest Neighbours algorithm.  The
model combines local metric learning, density‑tempered kernel weighting and
Dirichlet evidential aggregation to provide probability estimates and
uncertainty alongside predictions.

## Key features

* Learns a local Mahalanobis metric from nearby coreset points to adapt to
  anisotropic data distributions.
* Applies a density‑tempered kernel to reduce the influence of crowded regions.
* Aggregates neighbour information as evidence for each class and outputs a
  Dirichlet distribution, providing both class probabilities and uncertainty.
* Supports reduction of the training set via simple coreset construction to
  speed up predictions and reduce memory usage.

## Real‑world analogy

Imagine you arrive in a new town and want to decide which restaurant to try.
Rather than asking everyone, you pick a handful of locals from nearby streets
(*the coreset*), consider how people are clustered (*density*), adjust your
perception of distance based on the street layout (*local metric*), and weigh
their recommendations accordingly.  You also keep track of how confident you
are about their suggestions (*uncertainty mass*).  LAE‑KNN formalises this
intuition in a simple machine learning model.

## Basic usage

The library exposes a single class `LAEKNN`.  Below is a minimal example of
training and evaluating the model on the Iris dataset using scikit‑learn:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from laeknn import LAEKNN

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# initialise the model with default hyper‑parameters
clf = LAEKNN(per_class_centers=3, m=6, k_density=6, beta=0.4, tau=0.8, lam=1e-2)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, predictions))
```

## Credits

This package was developed by **Okunola Orogun** of **Endow Tech Limited**.

## License

This project is licensed under the MIT License – see the `LICENSE` file for
details.
